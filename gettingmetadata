from bs4 import BeautifulSoup
import urllib
p0 = urllib.urlopen('http://www.reuters.com/news/archive/turkey?view=page&page=1').read()
soup = BeautifulSoup(p0,'lxml')

content=soup.find_all("div",class_="story-content") #get the objects on the html page with type "div" and class "Story-content"


experiment={} #create a nested dictionary. the outermost key will be the article titles, one level below will be the date, and the link will be at the lowest
prefix="http://www.reuters.com/" #since the links in the html are not complete, you need to create this prefix to add to the beginning of everyone of them


for element in content:
    date = element.find(class_="timestamp").get_text()
    # Date already exists, add dictionary of link and summary to list of articles for the day
    if date in experiment.keys():
        experiment[date].append({"link": prefix + element.a["href"], "summary": element.p.get_text()})
    # Date does not exist, create a list of dictionaries for that date
    else:
        experiment[date]=[{"link": prefix + element.a["href"], "summary": element.p.get_text()}]
    
import json #use JSON to write the nested dictionaries into files, have a nice naming convention for each

with open("experiment1.json","w") as writeJSON:
    json.dump(experiment,writeJSON)
    
    
parsing_list=[] #list for p0,p1,p2,p3 etc
soup_list=[] #create a list with 65 entries so that you also have soup1,soup2,soup3 etc. 
content_list=range(0,67)

#list for the contents of each page. has 66 entries 

i=0
x=0

for x in range(68): #put the variables in soup_list (soup1,soup2 etc.)
    soup_list.append("soup"+str(x+1))

for i in range(68): 
    parsing_list.append("p"+str(i+1))

a=0

for a in range(0,67): #read the URLs and assign them to variables in soup_list (soup1,soup2 etc)
    parsing_list[a]=urllib.urlopen("http://www.reuters.com/news/archive/turkey?view=page&page=" + str(a+2) + "&pageSize=10").read()
    soup_list[a]=BeautifulSoup(parsing_list[a],"lxml")
    content_list[a]=soup_list[a].find_all("div",class_="story-content")    #get the objects on each page with type "div" and class "Story-content"
    
#each element in content_list contains 10 elements. it's a list of lists. this is for parsing mostly


titles_2={} #dictionary that will have the content (title, date, link) for pages 2-68

prefix="http://www.reuters.com" #since the links in the html are not complete, you need to create this prefix to add to the beginning of everyone of them

b=0

while b<len(content_list):
    for element in content_list[b]:
        
        titles_2[element.find(class_="timestamp").get_text()]={} #put the dates in the outermost layer so it's easier to sort by date later
        titles_2[element.find(class_="timestamp").get_text()]["link"]=prefix + element.a["href"] #put the links in the dict. label them as "link" so that whoever takes a look understands what it is 
        summary=element.p.get_text()
        titles_2[element.find(class_="timestamp").get_text()]["summary"]=summary #lastly, put the summary of the article
        
     
    b+=1       

print len(titles_2)
print len(experiment)
print experiment

import json #use JSON to write the nested dictionaries into files, have a nice naming convention for each

with open("Turkey_Pages_new.json","w") as writeJSON: #you may want to delete and redo this because I don't know if it writes a duplicate on it everytime
    json.dump(titles_2,writeJSON)

    
